# _誤差拡散学習法のサンプルプログラム_

_99/7/12 公開  
99/7/16 詳細追加  
99/8/6 一部修正  
99/8/19_ _論文追加  
99/10/27_ _経過報告_

---

[ED法サンプルプログラム（UNIX汎用、tgz、4KB）](https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/ed.tgz)

階層型神経回路網（ニューラルネットワーク）の教師あり学習アルゴリズムである誤差拡散学習法（ED法）のサンプルプログラムです。

比較のために用いたバックプロパゲーション法（ＢＰ法）のサンプルプログラムも示しておきます。

[慣性項ありBP法のサンプルプログラム（UNIX汎用、tgz、4KB）](https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/bp.tgz)

なお、これらのプログラムでは、階層型構造をリカレント型の一種とみなして計算しています。また、パラメータ入力の際には、単にリターンを押すとデフォルト値（括弧の中の値）が用いられるようになっていますので、実行の際にはリターンを連打すればＯＫです。また、X-Windowを用いてトータルエラーのグラフを表示しますので、実行の際にはDISPLAY環境変数をちゃんと設定するようお願いします。  
＃でないとコアダンプする(^^;

8/19追加

投稿予定の誤差拡散学習法の論文を公開しておきます。なお、このＥＤ法関連は、もう少ししたら、丸ごと抹消する予定です。

[誤差拡散学習法に関する論文（PDFファイル、575KB）](https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/edla.pdf)

あと、ＢＰ自体が判ってないと何の話かさっぱりわからんはずですので、[この辺](https://web.archive.org/web/19991124023203/http://www.gifu-nct.ac.jp/elec/deguchi/sotsuron/takemura/node13.html)見てみてください。なお、無断リンクです(^^;

10/27追加

既に表からリンク切っているので、これに気が付かれる方はかなり奇特な方です（笑）。これの経過報告をば。

投稿した論文ですが、現在、照会で帰ってきており、修正中です。年内にリターンする予定。なお、照会というのは、査読者のほうで疑問点がでたので直してねっということです。

主に出ている意見は、脳の学習理論か工学的に意味のあるニューロの理論か、どっちつかずなので、どうにかして・・という感じでしょうか？私は前者を推すので、下の図１、２の方を論文のメインにして、従来メインだった図３，４の方を考察で述べるというように論文内容を変更しています。実験結果の方は特に変わりません。

結局、すったもんだして、第３査読に回ると思いますが、通ってくれるとうれしいですねぇ。論文。

---

作者のコメント

　まず断っておきますが、冗談でやってる私の他のページと違い、このページはマジです。他は単なる趣味の「プログラミング」ですが、ここだけは趣味の「研究」のページです（まあ、こっちも趣味は趣味ですが(^^;）。

　さて、基本的なところから説明します。まず教師あり型の階層型神経回路網学習アルゴリズムというと、バックプロパゲーション法（ＢＰ法）が有名です。単純ＢＰが学習の対象とするのは、再帰構造を持たないフィードフォワード型の階層型神経回路網です。ただし、パーセプトロンと違い、ＢＰは中間層を有する３層以上の階層型神経回路網の学習が行えます。これより、線形分離可能な問題しか解けないという、第１次ニューロブームのパーセプトロンの問題点を解消し、第２次ニューロブームとなったわけです。

　さてさて。ここで、このＢＰ法の問題点が何なのか？ということを考えてみましょう。もちろん専門の方なら真っ先にＢＰの収束の遅さとか、中間層数などの各パラメータのヒューリスティック性などをあげるでしょうが、シミュレーション屋である私がこのＢＰに対して初めに思うのは、実際の神経系のシミュレーションとして考えるとＢＰはあまりにもヘンということです。パーセプトロンなら、これが小脳のモデルとみなされているように（マーの理論）、実際の神経系のモデルとしても自然です。ですが、ＢＰ以降の神経回路網の考え方は実際の神経系のモデルとしてはあまりにも不自然です。特にＢＰでの誤差情報が軸索を逆流しながら演算されるところが納得できません（ＢＰではあと、慣性項などの概念もヘン）。

　では実際の神経系では、中間層を持つ多層の神経網においては学習が行われていないのか？というと、それもやはりヘンです。中間層を学習しないタイプのパーセプトロン型学習法では、解ける問題が線形分離可能なもののみと非常に限られてしまいます。有名なところでは、排他的論理和（ＸＯＲ）も学習できません。

　では、教師あり学習自身が利用されていないのか？その可能性もありますが、心理学における強化理論などから、ある程度の教師あり学習が存在しているのは間違いありません。

　ということで、ここから導かれる結論。実際の脳が高度な学習能力を持っていることの類推から、非常に簡単で強力な階層型神経網の学習法則が存在し、それが実際の神経系で使われているはずということです。それでは、これはどんな方法なのでしょうか？ＢＰでしょうか？

　もちろん、ＢＰは有力な候補の一つですが、上に書いたように実際の神経系で使うにはＢＰは不自然すぎます。では・・・ということで導かれるのが、ここで述べるＥＤ（Error Diffusion）法です。ＢＰと同様、ＥＤでは誤差情報を後方に伝達することで教師あり学習しますが、誤差情報が軸索を逆流するという考えではなく、ＥＤ法では誤差情報（教師信号）が空間を介して入力側に伝播されるという考え方をとります。

　では、具体的に何が教師信号を伝達しているのでしょうか？それをＥＤ法では神経細胞外における、アミンなどの化学物質濃度と考えます。

\--------------------------------

補注

アミンは神経伝達物質の一つで、これに分類される神経伝達物質は、ノルアドレナリン（別名ノルエピネフリン）、ドーパミン、アドレナリン（別名エピネフリン）、セロトニンなどです。なお、この系の神経系としては、Ａ１０神経系（ドーパミン駆動）が有名ですね（アミン系としては各Ａ、Ｂ、Ｃ系があり、Ａ１０はＡ系の１０番目）。今までの神経回路網では、グルタミン酸やギャバのようなアミノ酸系神経系しかモデル化していませんでしたが、ＥＤ法ではこれらアミン系も神経回路網のモデルに含めます（この辺は、北野宏明氏編著「遺伝的アルゴリズム（２）」第７章などを参照）。

\--------------------------------

　さてさて、このアミン系ではシナプス末端がバリコシティという構造をとるため、情報がシナプスを介して一対一に伝達されるのではなく、ブロードキャスト的に情報が伝わるという特徴があります。隣接している各神経素子がアミン密度という同じ情報を共有するわけです。

　また、従来の神経回路網モデルでは、各軸索と神経を電線と処理装置というようにみなしていたため、実はネットワーク構造にのみ注目し神経素子が空間的にどこにあるか無視しています。しかし、ＥＤ法ではこのグローバル情報を考慮します。すなわちＥＤ法が仮定する神経回路では、隣接する神経細胞群（＝コラム）が共有情報（出力層の誤差信号）を持ち、コラムが巨大な一つの素子として働きます。出力層の教師信号をそのまま中間層でも用いるということです。

　ＥＤ法でもう一つ重要なことは、神経細胞には興奮性と抑制性の神経細胞があることに注目することです。もちろん、シナプスレベルの興奮性・抑制性は、従来のモデルでも考慮されてきましたが、ＥＤ法で重視するのはそれに加えて、「実際の神経系では、興奮性神経細胞の出力は全て興奮的に働き、抑制性神経細胞の出力は全て抑制的に働く」という事実です。従来モデルでは、シナプスレベルの興奮性、抑制性の区別しかしていませんでした。ＥＤ法ではそれに加えて、神経細胞レベルでも興奮性、抑制性の区別します。これが何を導くかというと、出力層の出力を上げ下げするための、中間層ウェイト変化方向が、出力層の誤差情報と神経細胞のタイプからローカルに一意に決まるということです。これにより、軸索レベルのバックプロックではなく、情報拡散（ブロードキャスト）によって多層回路網の学習が可能になるのです。

この２点がＥＤ法の重要な特徴です。

　結局ＥＤ法は、階層型神経網学習における単純山登り法です。ＢＰも同様に山登り法ですが、ＢＰがエラー関数の勾配による最急降下法であるのに対して、ＥＤ法は方向だけ考慮した特殊な山登り法です。概念的にはパーセプトロンに近いです。ＢＰが勾配情報で誤差を局所最小化するのに対して、ＥＤ法では回路構成を工夫することにより、階層型神経網でも確実に誤差が降下する方向に重みを変えます。ただし、神経素子はＥＤでも、ＢＰ同様、シグモイド関数を用いるアナログ非線型素子です。

---

ED法詳細

では、ED法に関する詳細を示します。

まず、階層型神経回路網の教師あり学習の基本である誤差逆伝播法（BP法）から話をはじめます。

このBPでは、神経素子の出力を以下のような式で求めます。

![1.gif (2877 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/1.gif)

ここで、k層の第iユニットへの入力の総和を i 、出力を o 、k-1層の第 i ユニットから k 層の第 j ユニットへの結合の重みを w とします。また、m 層を出力層とします。また、ここでは出力層のユニットを１つとします。そして、出力層における教師信号 y との２乗誤差

    ![2.gif (1299 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/2.gif)

を極小に持っていくためにBPでは最急降下法を用います。

   ![3.gif (3469 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/3.gif)

ここで、k = m すなわち出力層においては

    ![12.gif (724 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/12.gif)

です（なぜこれをわざわざd<sup>m</sup>と定義するかについてはEDの学習則の所で述べます）。

次に、k ≠ m の場合（出力層以外）、良く知られているようにBPでは![13.gif (218 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/13.gif)の項を k+1 層の誤差情報を元にして、入力層に向かって逆方向に計算します。これが誤差逆伝播法と呼ばれる所以ですが、私が問題とするのは、BPのこの項です。この軸索を逆流する情報逆伝播が実際の神経回路で用いられているとは思えません。そこで既に上で書いたように、これを神経細胞外における、アミンなどの化学物質の拡散により入力層側に伝わると考えます。

ここで (6) 式の![13.gif (218 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/13.gif)の項はすなわち、あるユニットの出力の変化による出力層での誤差の変化率です。そして、実際の神経回路において考えると、通常の神経では興奮性の神経細胞と抑制性の神経細胞があることがわかります。これは、ある細胞が興奮性であれば、この出力はどの細胞でも興奮性として働くということです。通常のニューロの計算においては、シナプスのレベルでは興奮性と抑制性の区別がありますが、神経細胞のレベルでは区別をしていません。この神経細胞単位での興奮性、抑制性の区別をすることにより、重みを上げれば良いのか下げれば良いのかが、簡単に求めることができます。以下の図において重みを変化させる方向について考えると、

![fig1.gif (2485 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/fig1.gif)![fig2.gif (2464 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/fig2.gif)

最終層における出力を上げたい場合、図1のように重みを変化させれば最終層の出力が必ず上昇します。ここでは最終層の細胞は興奮性としています。同様に図2のように重みを変化させれば最終層の出力が必ず下がります。ここで重みの変化方向について考えると、最終層から2層めまでについては同種の細胞間の結合を上げることにより、最終層の出力が上がり、異種の細胞間の結合を上げると最終層の出力が下がることがわかります。それ以降の層においても抑制の抑制以外において学習方向が一致します。  
  
実際の神経系とは食い違ってしまいますが、もし同種の間の結合を興奮性、異種の間の結合を抑制性とすると、図3、4に示すように、出力を上げたい場合、興奮性細胞からの結合を強め、出力を下げたい場合、抑制性細胞からの結合を強めれば良いことがわかります。これはどのようなネットワーク形状でも成り立ちます。  

![fig3.gif (2481 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/fig3.gif)![fig4.gif (2466 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/fig4.gif)

以下では３層構造でも問題なく動作するように、図3、4の方の方式を用いることとします（なお、２層目まででは、図1,2と図3,4はどちらも結果的に同じになります。実際は、論理回路の学習は中間層１層でＯＫなのでどちらでも良い）。

具体的な学習則ですが、 (7)式のd をアミンの濃度とみなし、興奮性細胞からの結合用と抑制性細胞からの結合用の２種類あるものとします。これを d<sup>+</sup>、d<sup>-</sup> とし、図3、4より以下のように定めます。  
  
まず出力層に関してですが、これは必ず興奮性なため、

もし y - o<sup>m</sup> > 0 ならば、(7) 式より

    d<sup>m+</sup> = y - o<sup>m</sup>

    d<sup>m-</sup> = 0

とし、y-o<sup>m</sup> < 0 ならば、

    d<sup>m+</sup> = 0

    d<sup>m-</sup> = o<sup>m </sup> \- y

とします。

次に最終層以外でのアミン濃度 d についてですが、これが拡散により伝わると考え、すべての層において

    ![6.gif (512 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/6.gif)

とします。

最終的に学習則は、出力層を含んだすべての層で等しく、興奮性細胞からの結合の場合、

    ![7.gif (951 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/7.gif)

であり、抑制性細胞からの結合の場合、

    ![8.gif (907 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/8.gif)

となります。ここで、関数 sign(x)は x の符号に応じて -1, 0, 1 を返す関数とします。このED法では必ずウェイトの絶対値が増える方向に学習が進行します。  
  
また誤差逆伝播法と同様に入出力関数としてシグモイド関数を用いているため、BP同様、

    ![9.gif (615 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/9.gif)

です。  
  
他に、興奮性細胞と抑制性細胞に分けたことにより、重みに制約条件が加わり、  
  
k-1 層 i 番めの細胞と k 層 j 番めの細胞が同種の場合

    ![10.gif (267 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/10.gif)

k-1 層 i 番めの細胞と k 層 j 番めの細胞が異種の場合

    ![11.gif (264 バイト)](/web/19991124023203im_/http://village.infoweb.ne.jp/~fwhz9346/11.gif)

という条件を満たす必要があります。  
  
また、通常のBPと異なり、入力ユニットに関しても興奮性細胞と抑制性細胞に分かれます。そのため、BPでの一つの入力が二つの興奮性細胞と抑制性細胞に対応し、対となる入力ユニットは同じ値を持つものとします。

---

　以下、ＥＤ法がどんなものなのかを知るために、今回のサンプルの実行結果を示しておきます。基本的に上で公開しているプログラム（orその変形版）で測定したものです。

＃あ～、良く見たら説明とプログラムでの変数名が違う(^^; 以下のα→上の説明でのεです。

ＸＯＲ問題

まず、中間層必須な問題のお約束としてＸＯＲ問題を解かせてみます。  
ＥＤ以外に、慣性項ありのＢＰ（以下、MBP）でも実験。

- 初期ウェイトを変えて100回測定した平均収束ステップ数を求める
- 収束判定 トータル誤差 < 0.1
- 10000ステップで計算打ち切り（タイムオーバー）
- タイムオーバーしたサンプルは平均から除外
- 重み変更は、そのつど（１パターン学習毎）に行う  
    

EDパラメータ

- 重み初期値0～1
- u0 0.4
- alpha 0.8

MBPパラメータ

- 重み初期値-0.05～0.05
- u0 0.4
- alpha 10
- moment 0.99  
    

収束にかかったステップ数（括弧内は、収束しないため計算が打ち切られた回数）

| 中間層素子数 | ED | MBP |
| --- | --- | --- |
| 2 | 339.75(37) | 172.66(7) |
| 4 | 180.72(1) | 145.46(0) |
| 8 | 75.14(0) | 136.96(0) |
| 16 | 28.00(0) | 127.05(0) |
| 32 | 8.26(0) | 117.17(0) |
| 64 | 5.97(0) | 93.84(14) |
| 128 | 5.09(0) | 収束しない（100） |
| 256 | 4.62(0) | 収束しない（100） |
| 512 | 4.47(0) | 収束しない（100） |

　ＥＤでは神経素子レベルで興奮性と抑制性を分けて考えるため、ＢＰと同等の構成を取るためにはＥＤでは中間層がＢＰの倍必要な点に注意してください。中間層数が少ないときはＢＰとほぼ同じ能力ですが、ＥＤは中間層ユニット数が増えるほど収束が早くなります。十分な中間ユニットがあればＥＤ法ではＸＯＲを５ステップで学習できることが分かります。

---

パリティチェック問題

ＸＯＲより学習の難しい問題として、パリティチェック問題を取り上げます。  
この問題の入力２ビットのときがＸＯＲ問題になります。  
線形分離を基本とする神経回路素子では、最も不得意な問題の一つです。  
やはり慣性項ありのＢＰと比較。

- 初期ウェイトを変えて100回測定した平均収束ステップ数を求める
- 収束判定 トータル誤差 < 0.1
- 10000ステップで計算打ち切り（タイムオーバー）
- タイムオーバーしたら、その時点で失敗
- 重み変更は、そのつど（１パターン学習毎）に行う  
    

EDパラメータ

- 重み初期値0～1
- u0 0.4
- alpha 0.8

MBPパラメータ

- 重み初期値-0.05～0.05
- u0 0.8
- alpha 10
- moment 0.99  
    

ＥＤで収束にかかったステップ数（打ち切りが一度でも発生したらＸ）

| 入力Bit数 | 中間数 8 | 中間数 16 | 中間数 32 | 中間数 64 | 中間数 128 | 中間数 265 | 中間数 512 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 2 | 75.14 | 28.00 | 8.26 | 5.97 | 5.09 | 4.62 | 4.47 |
| 3 | 162.79 | 81.64 | 22.53 | 11.22 | 10.04 | 8.65 | 7.94 |
| 4 | Ｘ | 344.92 | 111.17 | 56.20 | 30.54 | 22.09 | 17.09 |
| 5 | Ｘ | Ｘ | Ｘ | 330.94 | 217.62 | 171.46 | 80.18 |

MBPで収束にかかったステップ数（打ち切りが一度でも発生したらＸ）

| 入力Bit数 | 中間数 8 | 中間数 16 | 中間数 32 | 中間数 64 | 中間数 128 | 中間数 265 | 中間数 512 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 2 | 214.06 | 213.87 | 205.96 | 190.47 | Ｘ | Ｘ | Ｘ |
| 3 | 571.13 | 525.40 | 915.31 | Ｘ | Ｘ | Ｘ | Ｘ |
| 4 | 2656.42 | 1889.11 | Ｘ | Ｘ | Ｘ | Ｘ | Ｘ |
| 5 | Ｘ | Ｘ | Ｘ | Ｘ | Ｘ | Ｘ | Ｘ |

　ＢＰでは、問題に応じた中間層のユニット数最適値があるわけですが、ＥＤではそれがありません。中間層ユニット数を増やせば増やすほど収束が早くなります。実際の神経系で、いちいち問題に合わせて中間層のユニット数を調節しているわけありませんので、これはこれで納得。複雑な問題でも十分な数の中間ユニットさえあれば高速に学習できることがわかります。今回、時間がかかりすぎるため上の表に出せませんでしたが、中間層ユニットが１０００個あれば、８ビットパリティチェック（全２５６パターン）でも学習できることを確認しています。なお、その際の収束ステップ数は３００程度でした。

＃ＢＰでこのレベルの問題学習は無理でしょう。パラメータを試行錯誤的に調節すればＯＫかもしれませんが・・・・

　また、ＢＰが各パラメータに敏感なのに対して、ＥＤがかなり鈍感というのもあります。今回の各実験で、ＢＰの方では収束させるためにパラメータをいろいろ調節しなければいけないのに対して、ＥＤは全て同じ条件でＯＫでした。もちろん、今回一番難しい例として取り上げたＸＯＲやパリティ問題以外でも楽に学習できます。

結論。ＥＤ法では中間層数さえ大きく取っておけば、どんな論理関数でも数ステップで学習できます。

  
\-------------

**8/16 追加**

いろいろ調べてみたら、やはりBPでも８ビットパリティの学習ができるらしいことが判明。

Scott E. Fahlman and Chistian Lebiere: _The Cascade-Correlation Learning Architecture_, Advances in Neural Information Processing System II, pp. 524-532 (1989).

によると、通常ＢＰでも、中間ユニット16で2000ステップ程度で学習できるらしい。ただ、パラメータをチューニングする必要があるため、こちらでは収束させることはできず。

あと非常にたくさんある改良BP法のどれかを使えば、もちろんもっと早く収束させることができる。上で書いた学習アルゴリズムだと300ステップぐらい（中間層数がなんと４か５）。改良BPとして有名なQuickProcなら８０ステップぐらいで収束できるようだ。

---

手書き文字認識

今度はもう少し応用っぽい問題として、手書き文字認識をやってみます。

ＥＤは最急降下法でないのに加えて、中間層を多くとる必要があることから、汎化能力に問題があるのではないのか？という類推が成り立ちます。この問題でこの汎化能力の検証をしてみましょう。

- 手書き文字１０００文字学習させたあと、別の１０００文字サンプルの認識率を測定
- 各サンプルは、１６ｘ１６の２値画像（＝入力２５６ビット）
- サンプルは手書き文字（数字）で１０種類（＝出力１０ビット）、位置の補正などはしていない
- トータルエラーが１０以下になるまで学習（最大エラーで10000なので、１文字当り平均0.1%誤差）
- 1000ステップで計算打ち切り（タイムオーバー）
- 初期値を変えてサンプルＡセット学習＆Ｂセット認識１０回＋サンプルＢセット学習＆Ａセット認識１０回の平均
- 重み変更は、そのつど（１パターン学習毎）に行う  
    

EDパラメータ

- 重み初期値0～1
- u0 0.4
- alpha 0.8

MBPパラメータ

- 重み初期値-0.05～0.05
- u0 0.4
- alpha 0.8
- moment 0.9  
    

認識率（括弧内が収束にかかったステップ数、一度でもタイムオーバーで計算打ちきったらＸ）

| 中間層素子数 | ED | MBP |
| --- | --- | --- |
| 16 | 89.42%（21.0step） | 94.88%（225.8step） |
| 32 | 90.06%（11.1step） | 95.26%（114.1step） |
| 64 | 90.65%（9.1step） | 95.40%（74.5step） |
| 128 | 91.89%（9.2step） | 95.38%（107.5step） |
| 256 | 92.45%（9.2step） | Ｘ |
| 512 | 93.14%（9.3step） | Ｘ |

　予想通り、ＥＤ法では、ＢＰに比べて認識率が落ちます。ＢＰと比べて、よりローカルミニマに落ちやすく、ノイズを拾いやすいということでしょう。ただ、中間層素子数を変えたときの振る舞いが違います。良く知られているように、神経回路網では中間層はむやみに多くとればよいというわけではなく、問題に合わせた適当な数というのが決まっています。今回の例では、ＢＰの結果から６４個近辺が良いようです。ですが、それはあくまで学習アルゴリズムにＢＰを用いた時の話です。

　ＥＤでは、中間数は多ければ多いほど良いのです。多いほど収束速度が上がると共に、認識率も上がります。これは、出力層の誤差信号を中間層でも用いるために、中間層が増えれば増えるほど試行数が増えるのと同じことになるためだと思われます。確率での大数の法則と同じです。予想ですが、中間層素子数が無限個になるか、学習回数が無限大になると、汎化能力が、最適化したＢＰと同じになるのではないかと思います＜いまのところ、あくまで予想。

どちらにせよ、中間層数を増やせば認識率が93%（１０００文字学習後、他の１０００文字を判定して７０文字エラー）なので、まったくサンプル補正やパラメータ修正をしていないわりにはそこそこでしょう。学習収束速度は１０００文字学習に９ステップと、ＸＯＲ同様、非常に高速です。この辺も実際の神経系の学習則と考えれば納得。実際の神経系は非常に神経細胞数が多いわりに、各素子の動作速度が非常に遅い（数ms）ですので。逆に、実際の神経系なら、中間層数に余裕を持たせることによる、ロバスト性（回路にダメージが合ったときの耐久性）向上が、期待できます。

---

まとめ＆考察

　ＥＤ法は、実際の神経系の学習則として考え出された、階層型神経回路網の教師あり学習アルゴリズムです。アミン系と興奮性・抑制性の効果を新たに考慮しており、特に中間層素子数が大きな場合に、優れた収束性を持ちます。汎化能力は中間層素子数が少ない場合ＢＰに劣りますが、中間層素子数が多い場合ＢＰと同等です。論理回路の学習に関しては、ＢＰよりも優れています。何よりも、ＢＰ法よりシンプルで動作が安定しているアルゴリズムであり、実際の神経系でも利用可能であると仮定できる点が重要です。

　さて、このＥＤが実際に脳内のどこで使われているのか？ですが、アミン系で特に学習との関連が深い部位でもっともありうると思われます。ということは、報酬系と密接な関係を持つ前頭前野が最大の候補でしょう。ここは動作のプランニングを行うところですが、ドーパミン駆動系（Ａ９やＡ１０）などを介した強化学習が起こっているのではないかと思われます。ＥＤ法は文字認識などには弱いですが、論理学習には非常に優れた性能を発揮しますので、強化学習による行動学習には非常に向いていると思われます。

---

補足

強化学習による行動学習メカニズム（筆者予想）

1. 生体がある条件下にいる
2. 前頭前野において、各コラムが競合し最も出力の高いユニットに対応する行動が選択される
3. 選択された生物の行動（オペラント行動）が起こる
4. 行動の結果、正の強化信号もしくは負の強化信号を受ける
5. アミン系を介して強化信号が反応したコラムにフィードバックされる
6. ＥＤ法同様のメカニズムで対応行動コラムの反応率が増加、もしくは減少する

ということが起こると思われます。これを外部から観測すると、行動分析学での分析結果になると思われます。ということは、将来的には、これのシミュレーションが可能になるでしょう。究極的には、生物の行動を計算で求めることができるということですね＞計算行動学？

---

以上、まだ荒削りですが、ＥＤ法の可能性は示せているのではないかと思われます。これに関して何かご意見やご質問があるかたは、[金子](https://web.archive.org/web/19991124023203/mailto:kaneko@koma.jaeri.go.jp)の方までメールお願いします。

あ、あと書くの忘れてましたが、この方法は私オリジナルです。今のところここでしか情報を出していません。現在論文執筆中です。ここのＨＰに書くことで考えをまとめてたりします(^^;だれも見てないだろうけど、見てる可能性があると考えると多少はやる気がでますので（笑）。ということで、突っ込み大歓迎。

---

[ソフトウェアトップページに戻る](https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/index.htm)