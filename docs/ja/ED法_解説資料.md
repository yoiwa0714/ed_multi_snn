# ED法（誤差拡散学習法）解説資料

**Error Diffusion Learning Algorithm - EDLA**

## 目次
1. [ED法とは](#ed法とは)
2. [BP法との違い](#bp法との違い)
3. [ED法の理論的基盤](#ed法の理論的基盤)
4. [ED法の学習アルゴリズム](#ed法の学習アルゴリズム)
5. [実験結果](#実験結果)
6. [特徴と利点](#特徴と利点)
7. [参考文献](#参考文献)

---

## ED法とは

**誤差拡散学習法（Error Diffusion Learning Algorithm: ED法）** は、階層型神経回路網（ニューラルネットワーク）の教師あり学習アルゴリズムです。

### 開発者
- **金子勇** 氏によって開発されたオリジナルのアルゴリズム
- 1999年に発表、論文投稿中（当時）

### 基本コンセプト
ED法は、実際の脳神経系の構造と機能をより忠実に模倣した学習アルゴリズムです。従来の誤差逆伝播法（BP法）が持つ生物学的不自然さを解決し、より生物学的に妥当な学習メカニズムを提案しています。

---

## BP法との違い

### BP法の問題点（金子氏の指摘）

> **「実際の神経系のシミュレーションとして考えるとBPはあまりにもヘン」**

1. **軸索逆流問題**: 誤差情報が軸索を逆流しながら演算される点が生物学的に不自然
2. **慣性項の概念**: 生物学的根拠に乏しい
3. **複雑な勾配計算**: 実際の神経系では実現困難

### ED法のアプローチ

ED法では、以下の生物学的事実に基づいて問題を解決します：

1. **化学物質による情報拡散**
   - 誤差情報がアミン系神経伝達物質（ノルアドレナリン、ドーパミン、セロトニンなど）の濃度として空間を介して伝播
   - バリコシティ構造によるブロードキャスト的情報伝達

2. **興奮性・抑制性の区別**
   - 神経細胞レベルでの興奮性・抑制性の区別を導入
   - 従来のシナプスレベルの区別に加えて、細胞レベルでの区別を考慮

---

## ED法の理論的基盤

### 神経回路の基本構造

ED法では、以下の生物学的特徴を考慮します：

1. **アミン系神経系の役割**
   - グルタミン酸やGABAなどのアミノ酸系に加えて、アミン系神経系をモデル化
   - A10神経系（ドーパミン駆動）などの機能を組み込み

2. **コラム構造**
   - 隣接する神経細胞群（コラム）が共有情報を持つ
   - コラムが巨大な一つの素子として動作
   - 出力層の教師信号を中間層でも直接使用

3. **興奮性・抑制性細胞の役割**
   - 興奮性神経細胞：出力は全て興奮的に作用
   - 抑制性神経細胞：出力は全て抑制的に作用
   - この区別により、重み変化方向がローカルに決定可能

### 学習方向の決定原理

重みを変化させる方向は以下の原理に基づきます：

最終層における出力を上げたい場合、同種の細胞間の結合を上げることにより最終層の出力が上がり、異種の細胞間の結合を上げると最終層の出力が下がります。以下の図において重みを変化させる方向について考えてみましょう：

#### 図1・図2：基本的な重み変化方向

![図1: 出力上昇のための重み変化](fig1.gif)  
*図1: 最終層出力を上昇させるための重み変化方向*

![図2: 出力下降のための重み変化](fig2.gif)  
*図2: 最終層出力を下降させるための重み変化方向*

これらの図は、興奮性（○）と抑制性（●）の神経細胞の区別と、それらの間の結合重みの変化方向を示しています。図1では同種の細胞間結合を強化することで出力が上昇し、図2では異種の細胞間結合を強化することで出力が下降することを表現しています。

実際の神経系とは食い違ってしまいますが、もし同種の間の結合を興奮性、異種の間の結合を抑制性とすると、以下の図3・図4に示すように、出力を上げたい場合は興奮性細胞からの結合を強め、出力を下げたい場合は抑制性細胞からの結合を強めれば良いことがわかります：

#### 図3・図4：実装に適した重み変化方式

![図3: 興奮性細胞からの結合強化](fig3.gif)  
*図3: 興奮性細胞からの結合を強化することによる出力制御*

![図4: 抑制性細胞からの結合強化](fig4.gif)  
*図4: 抑制性細胞からの結合を強化することによる出力制御*

図3・図4は、より実装に適した形での重み変化方式を示しています。この方式では、出力を上げたい時は興奮性細胞からの結合を強化し、出力を下げたい時は抑制性細胞からの結合を強化することで、一貫した学習方向決定が可能になります。

この原理により、どのようなネットワーク形状でも一貫した学習が可能になります。3層構造でも問題なく動作するように、図3・図4の方式が採用されています（2層目まででは図1・図2と図3・図4は結果的に同じになります）。

---

## ED法のネットワーク構造

### 入力層

D法の入力層は興奮性ニューロンと抑制性ニューロンで1つのペアを構成し、そのペアで1つの入力(ピクセル)を担当します。また、興奮性ニューロンと抑制性ニューロンのペアには同じ値が入力されます。

### 出力層

1つの出力を1つの興奮性ニューロンが担当します。

## ED法の学習アルゴリズム

### BP法の基本式（比較のため）

まず、ED法を理解するために、従来のBP法の基本式を示します。

#### 神経素子の出力計算

**出力計算式：**
$$o_i^k = f(i_i^k) \quad \text{...(1)}$$

**入力の総和：**
$$i_j^k = \sum_{i} w_{ij}^k o_i^{k-1} \quad \text{...(2)}$$

**シグモイド関数：**
$$f(x) = \frac{1}{1+\exp(-2x/u_0)} \quad \text{...(3)}$$

#### 誤差関数

BP法では、出力層における教師信号 $y$ との2乗誤差を最小化します：
$$r = \frac{1}{2}(y - o^m)^2$$

> **注記**: 上記の誤差関数の式は、金子勇氏の元資料では「2.gif」として画像で示されていましたが、標準的なニューラルネットワークの2乗誤差関数として推測により補完しています。

#### BP法の重み更新

**基本重み更新式：**
$$\Delta w_{ij}^k = -\varepsilon \frac{\partial r}{\partial w_{ij}^k} \quad \text{...(4)}$$

**偏微分の展開：**
$$-\frac{\partial r}{\partial w_{ij}^k} = -\frac{\partial r}{\partial o_j^k} \frac{\partial o_j^k}{\partial i_j^k} \frac{\partial i_j^k}{\partial w_{ij}^k} \quad \text{...(5)}$$

$$= -\frac{\partial r}{\partial o_j^k} f'(o_j^k)o_i^{k-1} \quad \text{...(6)}$$

**出力層での誤差項：**
$$d^m = -\frac{\partial r}{\partial o^m} = y - o^m \quad \text{...(7)}$$

### ED法の学習則

ED法では、上記のBP法の概念を生物学的に妥当な形に変更します。

#### 1. アミン濃度の定義

出力層における誤差から、興奮性・抑制性それぞれのアミン濃度を定義：

**出力層（必ず興奮性）における誤差信号：**
- $y - o^m > 0$ の場合：$d^{m+} = y - o^m$, $d^{m-} = 0$
- $y - o^m < 0$ の場合：$d^{m+} = 0$, $d^{m-} = o^m - y$

#### 2. 情報拡散（アミン濃度の伝播）

アミン濃度が空間を介して全ての層に拡散：
- $d^{k+} = d^{m+}$（全ての層で同じ値）
- $d^{k-} = d^{m-}$（全ての層で同じ値）

#### 3. 重み更新則

**興奮性細胞からの結合：**
$$\Delta w_{ij}^k = \varepsilon d_j^{k+} f'(o_j^k) o_i^{k-1} \text{sign}(w_{ij}^k)$$

**抑制性細胞からの結合：**
$$\Delta w_{ij}^k = \varepsilon d_j^{k-} f'(o_j^k) o_i^{k-1} \text{sign}(w_{ij}^k)$$

> **注記**: 上記の重み更新式において、元資料では抑制性細胞の式で「sin(w_{ij}^k)」と記載されていましたが、文脈から「sign(w_{ij}^k)」の誤記と判断し修正しています。また、変数の表記を一貫性のため統一しています。

#### 4. シグモイド関数の微分

BP法と同様にシグモイド関数を使用：
$$f'(x) = f(x)(1 - f(x))$$

#### 5. 重み制約条件

**同種細胞間（興奮性同士、抑制性同士）：**
$$w_{ij}^k > 0$$

**異種細胞間（興奮性と抑制性）：**
$$w_{ij}^k < 0$$

### 特徴

ED法の重要な特徴：

- **必ずウェイトの絶対値が増える方向に学習が進行**
- **シグモイド関数を入出力関数として使用**
- **概念的にはパーセプトロンに近い単純山登り法**
- **生物学的に妥当な情報伝播メカニズム**

### BP法とED法の根本的違い

**BP法の問題点：**

- 誤差情報が軸索を逆流（生物学的に不自然）
- 複雑な勾配計算が必要
- パラメータ調整が困難

**ED法の解決策：**

- アミン系神経伝達物質による空間的情報拡散
- 興奮性・抑制性細胞の区別による単純な学習方向決定
- ローカルな情報のみで重み更新が可能

> **注記**: 上記の「BP法とED法の根本的違い」の構成は、金子勇氏の元資料の内容を基に、理解しやすくするため編集者が整理・要約したものです。

---

## 実験結果

### XOR問題（中間層必須問題）

| 中間層素子数 | ED収束ステップ | MBP収束ステップ |
|-------------|---------------|----------------|
| 32          | 8.26          | 117.17         |
| 64          | 5.97          | 93.84          |
| 128         | 5.09          | 収束しない      |
| 256         | 4.62          | 収束しない      |

**結果**: 十分な中間ユニットがあればED法ではXORを5ステップで学習可能

### パリティチェック問題

**ED法の特徴：**
- 中間層ユニット数を増やすほど収束が高速化
- 問題に応じた最適ユニット数の調整が不要
- 8ビットパリティ（256パターン）も300ステップで学習可能

**BP法との比較：**
- BP法は問題に応じた中間層数の最適化が必要
- パラメータに敏感で調整が困難
- 複雑な問題では収束しない場合が多い

### 手書き文字認識

**実験条件：**
- 16×16の2値画像（入力256ビット）
- 手書き数字10種類（出力10ビット）
- 1000文字学習後、別の1000文字で認識率測定

| 中間層素子数 | ED認識率 | ED収束ステップ | MBP認識率 | MBP収束ステップ |
|-------------|----------|----------------|-----------|----------------|
| 128         | 91.89%   | 9.2           | 95.38%    | 107.5          |
| 256         | 92.45%   | 9.2           | 収束しない  | -              |
| 512         | 93.14%   | 9.3           | 収束しない  | -              |

**結果**：
- ED法は収束速度が非常に高速（9ステップ）
- 中間層数を増やすほど認識率が向上
- BP法は汎化能力に優れるが、収束が困難

---

## 特徴と利点

### ED法の主要特徴

1. **生物学的妥当性**
   - 実際の神経系で実現可能なメカニズム
   - アミン系神経伝達物質による情報拡散
   - 興奮性・抑制性細胞の区別

2. **シンプルなアルゴリズム**
   - BP法より単純で理解しやすい
   - パラメータ調整が容易
   - 動作が安定

3. **優れた収束性**
   - 中間層数が多いほど高速収束
   - 論理回路学習に特に優秀
   - パラメータに対してロバスト

4. **スケーラビリティ**
   - 中間層数に制限なし
   - 複雑な問題にも対応可能
   - ハードウェア実装に適している

### 適用分野

**得意分野：**
- 論理回路の学習
- パターン分類
- 強化学習による行動学習

**期待される応用：**
- 前頭前野での行動プランニング
- 報酬系との連携による強化学習
- 計算行動学への応用

---

## まとめ

ED法は、実際の神経系の学習則として考え出された革新的なアルゴリズムです。BP法の生物学的不自然さを解決し、より単純で効率的な学習を実現します。

### 主要な貢献

1. **生物学的に妥当な学習アルゴリズムの提案**
2. **アミン系神経系の機能をモデル化**
3. **興奮性・抑制性の効果を考慮した学習則**
4. **優れた収束性と安定性の実現**

ED法は、人工知能と計算神経科学の橋渡しとなる重要な手法として、今後の発展が期待されます。

> **重要な注記**: この解説資料における金子勇氏のオリジナル情報と編集・補完部分の区別について、以下の点にご注意ください：
> 
> - **オリジナル情報**: 金子勇氏の1999年のウェブサイトからの直接引用および実験結果
> - **編集・補完部分**: 数式の一部修正、構成の整理、説明の補足は編集者による
> - **推測による補完**: 画像として表示されていた数式の復元は標準的なニューラルネットワーク理論に基づく推測
> 
> オリジナルの研究成果と理論は全て金子勇氏によるものです。

---

## 参考文献

1. 金子勇, "誤差拡散学習法のサンプルプログラム", 1999
   - URL: https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/ed.htm
2. 金子勇, "誤差拡散学習法に関する論文", 1999（投稿中）
3. 北野宏明編著, "遺伝的アルゴリズム（２）", 第7章（アミン系神経系について）

## 画像・図表の出典と著作権

### 図1～図4の著作権情報

本資料に含まれる図1～図4（fig1.gif～fig4.gif）は、金子勇氏による1999年のオリジナル資料から引用：

- **出典**: 金子勇氏ウェブサイト「誤差拡散学習法のサンプルプログラム」(1999年)
- **アーカイブURL**: https://web.archive.org/web/19991124023203/http://village.infoweb.ne.jp:80/~fwhz9346/ed.htm
- **著作権**: 金子勇氏
- **使用目的**: 学術的解説・教育目的での引用

### 図の内容説明

- **図1**: 最終層出力上昇のための重み変化方向（興奮性・抑制性の基本概念）
- **図2**: 最終層出力下降のための重み変化方向（興奮性・抑制性の基本概念）  
- **図3**: 興奮性細胞からの結合強化による出力制御（実装向け方式）
- **図4**: 抑制性細胞からの結合強化による出力制御（実装向け方式）

これらの図は、ED法の核心である「興奮性・抑制性細胞の区別による学習方向決定」メカニズムを視覚的に説明する貴重な資料です。

---

**注意**: この解説資料は、金子勇氏のオリジナル資料（1999年）を基に作成されています。ED法は金子氏のオリジナル手法であり、ここで示された理論と実験結果は氏の研究成果です。

**編集・補完に関する免責事項**: 
- 元資料で画像として表示されていた数式は、標準的なニューラルネットワーク理論に基づいて復元しています
- 一部の表記統一や構成整理は編集者によるものです
- オリジナルの研究内容や実験結果の解釈については、必ず元資料をご参照ください

**作成日**: 2025年6月28日  
**基づく資料**: 金子勇氏ウェブサイト（1999年、Web Archive版）  
**編集者注**: 数式復元および構成整理を含む
