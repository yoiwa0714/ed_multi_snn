"""
PyTorchæ¨™æº–æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸMNIST/Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†

æ±ç”¨æ€§ã¨ed_multi_snn.prompt.mdæº–æ‹ ã®æœ€é©åŒ–ã‚’é‡è¦–
ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æ©Ÿèƒ½çµ±åˆã«ã‚ˆã‚‹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š

ä½œæˆè€…: ED-SNNé–‹ç™ºãƒãƒ¼ãƒ   
ä½œæˆæ—¥: 2025å¹´9æœˆ28æ—¥
"""

import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
from torch.utils.data import DataLoader, Dataset
from typing import Tuple, Optional, List, Dict, Any
import os
import time

from .profiler import profile_function, TimingContext


class MNISTDatasetManager:
    """
    PyTorchæ¨™æº–æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ãŸMNIST/Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
    
    ç‰¹å¾´:
    - PyTorchã®æ¨™æº–transformsã‚’ä½¿ç”¨
    - åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†
    - ED-SNNå‘ã‘ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ±åˆ
    """
    
    def __init__(
        self,
        dataset_type: str = 'MNIST',
        data_dir: str = './data',
        batch_size: int = 32,
        normalize: bool = True,
        download: bool = True
    ):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
        
        Parameters:
        -----------
        dataset_type : str
            'MNIST' or 'FashionMNIST'
        data_dir : str
            ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        batch_size : int
            ãƒãƒƒãƒã‚µã‚¤ã‚º
        normalize : bool
            æ­£è¦åŒ–å®Ÿè¡Œãƒ•ãƒ©ã‚°
        download : bool
            è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ãƒ©ã‚°
        """
        self.dataset_type = dataset_type
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.normalize = normalize
        
        # ãƒ‡ãƒ¼ã‚¿å¤‰æ›å®šç¾©
        self.transform = self._create_transform()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠ
        if dataset_type.upper() == 'MNIST':
            self.dataset_class = torchvision.datasets.MNIST
        elif dataset_type.upper() == 'FASHIONMNIST':
            self.dataset_class = torchvision.datasets.FashionMNIST
        else:
            raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_type}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–
        self._initialize_datasets(download)
        
        print(f"âœ… {dataset_type}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–å®Œäº†")
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(self.train_dataset)}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(self.test_dataset)}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    
    def _create_transform(self) -> transforms.Compose:
        """ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆ"""
        transform_list = [transforms.ToTensor()]
        
        if self.normalize:
            # MNIST/Fashion-MNISTæ¨™æº–æ­£è¦åŒ–
            transform_list.append(
                transforms.Normalize((0.1307,), (0.3081,))
            )
        
        return transforms.Compose(transform_list)
    
    @profile_function("dataset_initialization")
    def _initialize_datasets(self, download: bool):
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆæœŸåŒ–"""
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        self.train_dataset = self.dataset_class(
            root=self.data_dir,
            train=True,
            transform=self.transform,
            download=download
        )
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        self.test_dataset = self.dataset_class(
            root=self.data_dir,
            train=False,
            transform=self.transform,
            download=download
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        self.test_loader = DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
    
    @profile_function("get_batch_data")
    def get_batch(self, train: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—
        
        Parameters:
        -----------
        train : bool
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ©ã‚°
            
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray]
            (images, labels) - ED-SNNå½¢å¼
        """
        loader = self.train_loader if train else self.test_loader
        
        for batch_images, batch_labels in loader:
            # PyTorchãƒ†ãƒ³ã‚½ãƒ« â†’ NumPyé…åˆ—
            images = batch_images.numpy()
            labels = batch_labels.numpy()
            
            # å½¢çŠ¶å¤‰æ›: (batch, 1, 28, 28) â†’ (batch, 784)
            images = images.reshape(images.shape[0], -1)
            
            # one-hot ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            labels_onehot = np.eye(10)[labels]
            
            return images, labels_onehot
    
    @profile_function("get_single_sample")
    def get_single_sample(self, index: int, train: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        
        Parameters:
        -----------
        index : int
            ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        train : bool
            è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ©ã‚°
            
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray]
            (image, label) - ED-SNNå½¢å¼
        """
        dataset = self.train_dataset if train else self.test_dataset
        
        image, label = dataset[index]
        
        # Tensor â†’ NumPy, å½¢çŠ¶å¤‰æ›
        image_np = image.numpy().reshape(-1)  # (784,)
        label_onehot = np.eye(10)[label]
        
        return image_np, label_onehot
    
    @profile_function("create_small_dataset")
    def create_small_dataset(self, n_samples: int = 100) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        ãƒ‡ãƒãƒƒã‚°ç”¨å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        
        Parameters:
        -----------
        n_samples : int
            ã‚µãƒ³ãƒ—ãƒ«æ•°
            
        Returns:
        --------
        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]
            (train_X, train_y, test_X, test_y)
        """
        print(f"ğŸ” å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ (n={n_samples})")
        
        train_X, train_y = [], []
        test_X, test_y = [], []
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        for i in range(min(n_samples, len(self.train_dataset))):
            image, label = self.get_single_sample(i, train=True)
            train_X.append(image)
            train_y.append(label)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_size = min(n_samples // 5, len(self.test_dataset))  # 20%ã‚’ãƒ†ã‚¹ãƒˆç”¨
        for i in range(test_size):
            image, label = self.get_single_sample(i, train=False)
            test_X.append(image)
            test_y.append(label)
        
        return (np.array(train_X), np.array(train_y), 
                np.array(test_X), np.array(test_y))
    
    def get_class_names(self) -> List[str]:
        """ã‚¯ãƒ©ã‚¹åå–å¾—"""
        if self.dataset_type.upper() == 'MNIST':
            return [str(i) for i in range(10)]
        elif self.dataset_type.upper() == 'FASHIONMNIST':
            return ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
        return []
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±å–å¾—"""
        return {
            'dataset_type': self.dataset_type,
            'train_size': len(self.train_dataset),
            'test_size': len(self.test_dataset),
            'batch_size': self.batch_size,
            'input_shape': (784,),  # 28x28 flattened
            'num_classes': 10,
            'class_names': self.get_class_names(),
            'normalize': self.normalize
        }


class EDSNNDataProcessor:
    """
    ED-SNNç”¨ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹
    
    PyTorchãƒ‡ãƒ¼ã‚¿ã¨ED-SNNå½¢å¼ã®åŠ¹ç‡çš„ãªå¤‰æ›
    """
    
    @staticmethod
    @profile_function("normalize_images")
    def normalize_for_ed_snn(images: np.ndarray, method: str = 'minmax') -> np.ndarray:
        """
        ED-SNNç”¨ç”»åƒæ­£è¦åŒ–
        
        Parameters:
        -----------
        images : np.ndarray
            å…¥åŠ›ç”»åƒ (batch_size, 784)
        method : str
            æ­£è¦åŒ–æ–¹æ³• ('minmax', 'standard', 'sigmoid')
            
        Returns:
        --------
        np.ndarray
            æ­£è¦åŒ–æ¸ˆã¿ç”»åƒ [0, 1]
        """
        if method == 'minmax':
            # Min-Maxæ­£è¦åŒ– [0, 1]
            img_min = np.min(images, axis=1, keepdims=True)
            img_max = np.max(images, axis=1, keepdims=True)
            return (images - img_min) / (img_max - img_min + 1e-8)
        
        elif method == 'standard':
            # Z-scoreæ­£è¦åŒ– â†’ Sigmoidå¤‰æ›
            img_mean = np.mean(images, axis=1, keepdims=True)
            img_std = np.std(images, axis=1, keepdims=True)
            z_scores = (images - img_mean) / (img_std + 1e-8)
            return 1.0 / (1.0 + np.exp(-z_scores))
        
        elif method == 'sigmoid':
            # Sigmoidæ­£è¦åŒ–
            return 1.0 / (1.0 + np.exp(-images * 6.0))
        
        else:
            raise ValueError(f"æœªå¯¾å¿œã®æ­£è¦åŒ–æ–¹æ³•: {method}")
    
    @staticmethod
    @profile_function("augment_data")  
    def augment_for_training(images: np.ndarray, noise_level: float = 0.05) -> np.ndarray:
        """
        å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ
        
        Parameters:
        -----------
        images : np.ndarray
            å…¥åŠ›ç”»åƒ
        noise_level : float
            ãƒã‚¤ã‚ºãƒ¬ãƒ™ãƒ«
            
        Returns:
        --------
        np.ndarray
            æ‹¡å¼µæ¸ˆã¿ç”»åƒ
        """
        # ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºè¿½åŠ 
        noise = np.random.normal(0, noise_level, images.shape)
        augmented = np.clip(images + noise, 0, 1)
        
        return augmented


def benchmark_dataset_loading():
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿æ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿æ€§èƒ½æ¸¬å®š")
    print("=" * 50)
    
    # MNIST
    with TimingContext("mnist_loading"):
        mnist_manager = MNISTDatasetManager('MNIST', batch_size=64)
        train_X, train_y = mnist_manager.create_small_dataset(1000)
    
    # Fashion-MNIST
    with TimingContext("fashion_mnist_loading"):
        fashion_manager = MNISTDatasetManager('FashionMNIST', batch_size=64)
        fashion_X, fashion_y = fashion_manager.create_small_dataset(1000)
    
    print(f"MNISTå½¢çŠ¶: {train_X.shape}, {train_y.shape}")
    print(f"Fashion-MNISTå½¢çŠ¶: {fashion_X.shape}, {fashion_y.shape}")
    
    from .profiler import profiler
    print(profiler.get_performance_report())


if __name__ == "__main__":
    benchmark_dataset_loading()